# 数据标准化



## **数据标准化的常见方法**

### **1、Min-Max (Minimum and Maximum normalisation)**

对观测值$x_j$进行如下处理:
$$
x_j^{{\rm{new }}} = \frac{{{x_j} - \min (X)}}{{\max (X) - \min (x)}}
$$
这里的 $\min (x)$表示变量 $x$的最小值， $\max(X)$表示变量$X$ 的最大值， $ x_j^{{\rm{new }}} $表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in [0,1]$。

![Min-Max](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Min-Max.png)

### **2、Mean-Std (Mean and standard deviation normalisation)**

对观测值$x_j$进行如下处理:

$$
x_j^{{\rm{new }}} = \frac{{{x_j} - {\mathop{\rm mean}\nolimits} (X)}}{{\sqrt {{\mathop{\rm std}\nolimits} (X)} }}
$$
这里的 ${{\mathop{\rm mean}\nolimits} (X)}$ 表示变量 $X$的均值， ${\sqrt {{\mathop{\rm std}\nolimits} (X)} }$表示变量 $X$的标准差，  $ x_j^{{\rm{new }}} $表示标准化之后的数据。和上面不一样的是， $ x_j^{{\rm{new }}} $的取值范围并不在区间 [0,1] ，而是$ x_j^{{\rm{new }}}  \in R$。

![Mean-Std](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean-Std.png)

### **3、Mean-IQR( Median and the IQR normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = \frac{{{x_j} - {\mathop{\rm median}\nolimits} (X)}}{{{\mathop{\rm IQR}\nolimits} (X)}}}
$$
这里的${{\mathop{\rm median}\nolimits} (X)}$表示变量 X的中位数， ${{\mathop{\rm IQR}\nolimits} (X)}$表示变量 X的四分位距 (Interquartile range)，$ x_j^{{\rm{new }}} $表示标准化之后的数据。其中四分位距 ${{\mathop{\rm IQR}\nolimits} (X) = {Q_3}(X) - {Q_1}(X)}$，也就是75%分位数和25%分位数之间的距离，具体见图0. 不难看出，$ x_j^{{\rm{new }}}  \in R$。

![img](https://mmbiz.qpic.cn/sz_mmbiz_jpg/EBka0dZichyyD0D7GFKWd6quFHuhPVZiaABXLcBVBNq0wM8HicXAGtNWnNu0Se4UJKiap2pn3iatuotCGd96zbIQMNw/640?wx_fmt=jpeg)

图0. 四分位距IQR示意图

![Mean-mean_iqr](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean-mean_iqr.png)

### **4、Median-MAD(Median and median absolute deviation normalisation )**

对观测值 $x_j$进行如下处理:

$$
{x_j^{new} = \frac{{{x_j} - {\mathop{\rm median}\nolimits} (X)}}{{MAD(X)}}}
$$
这里的 median(X)  表示变量 X的中位数， ${{\mathop{\rm MAD}\nolimits} (X) = {\mathop{\rm median}\nolimits} (|X - {\mathop{\rm median}\nolimits} (X)|)}$表示变量 X的中位数的中位数(这个比较拗口哈哈哈)， $ x_j^{{\rm{new }}}$表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in R$ 。

![Mean-mad](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean-mad.png)

### **5、Mean (Mean normalisation)**

对观测值 $x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = {x_j} - {\mathop{\rm mean}\nolimits} (X)}
$$
这里的 mean(X) 表示变量 $X$的均值，$ x_j^{{\rm{new }}}$表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in R$ 。。这种标准化操作又叫做中心化(Centralisation)，可以看作是Mean-Std标准化的特殊情况。

![Mean](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean.png)

### **6、Ordinal (Ordinal normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{new} = \frac{{{\mathop{\rm rank}\nolimits} \left( {{x_j}} \right) - 1}}{{{\mathop{\rm Max}\nolimits} R(X) - 1}}}
$$
这里的${{\mathop{\rm rank}\nolimits} \left( {{x_j}} \right)}$表示观测值 $x_j$在变量 X 的所有观测值中按照从小到大排序之后的相对位置(rank)， MaxR(X)表示变量 X的所有观测值的最高排序(因为可能有重复值的出现，所以这个值并不一定等于观测值总数 $n$ )，$ x_j^{{\rm{new }}} $表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in [0,1]$ 。

![ordinal](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\ordinal.png)

### **7、Frequency (Frequency normalisation)**

对观测值 $x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = \frac{{{\mathop{\rm count}\nolimits} \left( {{x_j}} \right)}}{{\sum\limits_{k = 1}^n {{\mathop{\rm count}\nolimits} } \left( {{x_k}} \right)}}}
$$
这里的${{\mathop{\rm count}\nolimits} \left( {{x_j}} \right)}$ 表示观测值 $x_j$出现的次数，${{\mathop{\rm count}\nolimits} \left( {{x_j}} \right)}$ 表示标准化之后的数据，在这里其实就是 $x_j$出现的频率。不难看出，$ x_j^{{\rm{new }}}  \in [0,1]$ 。

![frequency](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\frequency.png)

### **8、PS (Pareto Scaling normalisation)**

对观测值 $x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = \frac{{{x_j} - {\mathop{\rm mean}\nolimits} (X)}}{{\sqrt {{\mathop{\rm std}\nolimits} (X)} }}}
$$
这里的 ${{\mathop{\rm mean}\nolimits} (X)}$表示变量 $X$的均值， ${\sqrt {{\mathop{\rm std}\nolimits} (X)} }$ 表示变量 X的标准差的平方根， $ x_j^{{\rm{new }}}$表示标准化之后的数据。可以看出，和**Mean-Std**标准化唯一不一样的是，在分母处我们使用了变量 $X$的标准差的平方根。不难看出，$ x_j^{{\rm{new }}}  \in R$。像我这样的杠精可能就会问了：为什么我们要使用这种奇葩的标准化方法呢？van den Berg et al.(2006)[18]给出的解释是：

"The method improves the representation of lower concentrated features while minimising the contribution of noise in the data. Further, it removes the limitation of the unit variance in contrast to the Mean-Std method and keeps the structure of the data partially intact."

简言之，这个标准化方法可以提升“lower concentrated feature”的影响力、并且同时减少噪声的干扰。若要有更加深刻的理解，请参考原文[18].

![pareto_scaling](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\pareto_scaling.png)

### **9、VSS (Variable Stability Scaling normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = \frac{{{x_j} - {\mathop{\rm mean}\nolimits} (X)}}{{{\mathop{\rm std}\nolimits} (X)}} \cdot \frac{{{\mathop{\rm mean}\nolimits} (X)}}{{{\mathop{\rm std}\nolimits} (X)}}}
$$
这里的 mean(X)表示变量 X的均值， std(X)表示变量 X的标准差， $ x_j^{{\rm{new }}} $ 表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in R$ 。此外，和**Mean-Std**标准化唯一不一样的是: 我们在后面乘了一项我们称之为变异系数(coefficient of variation)的项 ${\frac{{{\mathop{\rm mean}\nolimits} (X)}}{{{\mathop{\rm std}\nolimits} (X)}}}$。像我这样的杠精可能就会问了：为什么我们要使用这种奇葩的标准化方法呢？van den Berg et al.(2006)[18]给出的解释是：

"The coefficient of variation gives higher importance to those features which have a small standard deviation and lower importance to those that have a large standard deviation. "

简言之，这个标准化方法可以提升“small standard deviation”的变量的影响力、并且同时减少“large standard deviation”的变量的影响力。若要有更加深刻的理解，请参考原文[18].

![variable_stability_scaling](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\variable_stability_scaling.png)

### **10、PT (Power Transformation normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{{\rm{new }}} = \sqrt {{x_j} - \min (X)}  - \sqrt {{\mathop{\rm mean}\nolimits} (X)} }
$$
这里的${{\mathop{\rm mean}\nolimits} (X)}$表示变量$X$的均值， $\min (X)$表示变量 $X$的最小值， $ x_j^{{\rm{new }}}$表示标准化之后的数据。不难看出，$ x_j^{{\rm{new }}}  \in R$。像我这样的杠精可能就会问了：为什么我们要使用这种奇葩的标准化方法呢？

"This method transforms the data into homoscedasticity by reducing the effects of heteroscedasticity."

简言之，这个标准化方法是为了减少不同变量之间异方差性的影响。若要有更加深刻的理解，请参考原文[20].

**小结1**：上述的几种数据标准化方法中，**Mean-Std**, **Mean、Ordinal、Frequency、PS、VSS、PT**具有以下共同特点[16]:

These methods help to reduce the effect of outliers from the data but do not overcome the problem of dominant features entirely.

简言之，这几种方法可以有效减少异常值(outlier)的影响，但是不能解决"dominant feature"的问题(也就是说，异常检测或者其他机器学习算法的最终结果主要由少数的几个domninant feature控制，有点垄断那种意思。)

![power_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\power_transformation.png)

### **11、Max (Maximum normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{new} = \frac{{{x_j}}}{{\max (|X|)}}}
$$
这里的${\max (|X|)}$表示变量 X的绝对值的最大值， $ x_j^{{\rm{new }}} $表示标准化之后的数据。不难看出， $ x_j^{{\rm{new }}}  \in [-1,1]$。此外，这种方法又叫做 **Decimal Scaling Normalisation。**

![maximum_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\maximum_transformation.png)

**小结2**：上述的几种数据标准化方法中，**Min-Max, Max**具有以下共同特点[16]:

"These normalisation methods are useful for preserving the relationships among the original input data, unlike normalisation methods which are based on a mean and standard deviation of the data as these values may vary with time. The main drawback of this data normalisation is its sensitivity towards outliers as well as extreme values present in un-normalised data."

简言之，这几种方法可以**较好地保留原始数据之间的关系，但是对异常值、极端值比较敏感**。



### **12、LSN(Logistic Sigmoid Normalisation)**

对观测值$x_j$进行如下处理:

$$
{x_j^{new} = \frac{1}{{1 + {e^{ - {z_j}}}}}}
$$
其中，

$$
{{z_j} = \frac{{{x_j} - {\mathop{\rm mean}\nolimits} (X)}}{{\eta  \cdot {\mathop{\rm std}\nolimits} (X)}}}
$$
这里的 ${{\mathop{\rm mean}\nolimits} (X)}$表示变量 $X$的均值，${{\mathop{\rm std}\nolimits} (X)}$表示变量$X$的标准差, $\eta $是一个常数，一般取(0,3]。有点难看出（哈哈哈），$ x_j^{{\rm{new }}}  \in (0,1)$。

![logistic_sigmoid_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\logistic_sigmoid_transformation.png)

### **13、HTN(Hyperbolic Tangent Normalisation)**

对观测值 $x_j$进行如下处理:

$$
{x_j^{new} = \frac{{1 - {e^{ - {z_j}}}}}{{1 + {e^{ - {z_j}}}}}}
$$
其中，

$$
{{z_j} = \frac{{{x_j} - {\mathop{\rm mean}\nolimits} (X)}}{{{\mathop{\rm std}\nolimits} (X)}}}
$$
这里的 ${{\mathop{\rm mean}\nolimits} (X)}$表示变量 X的均值，${{\mathop{\rm std}\nolimits} (X)}$表示变量 $X$的标准差。有点难看出，$ x_j^{{\rm{new }}}  \in (0,1)$。

![hyperbolic_tangent_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\hyperbolic_tangent_transformation.png)

**小结3**：上述的几种数据标准化方法中，**LSN, HTN**具有以下共同特点[16]:

"This normalisation preserves the significance of the normal data while dealing with the outliers. The values that lie within a standard deviation of the mean of the feature are mapped to almost linear region whereas outliers, as well as extreme values, are mapped along the tails of the range by the given function. "

简言之，这种数据标准化方法对异常值不敏感。最后，准备简单提一下一种比较变态的数据标准化方法，TBN (Tanh Based Normalisation)，详情请参考[16,20]。一般看见这种变态操作，或者迷惑操作，我就会首先怀疑它是不是由搞deep learning的人提出来的。

对于上述一部分数据标准化方法的优缺点，van den Berg et al.(2006)[18]将其总结如图2所示。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/EBka0dZichyyD0D7GFKWd6quFHuhPVZiaAXialszAVaDGqnus2xk3gAo8YuQY3tdYUgawI3re22lNplk9q5Ccia8LQ/640?wx_fmt=png&from=appmsg)

## 代码实现

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def min_max_normalization(X):
    return (X - X.min()) / (X.max() - X.min())

def mean_std_normalization(X):
    return (X - X.mean()) / X.std()

def mean_iqr_normalization(X):
    return (X - X.median()) / (X.quantile(0.75) - X.quantile(0.25))

def median_mad_normalization(X):
    return (X - X.median()) / np.median(np.abs(X - X.median()))

def mean_normalization(X):
    return X - X.mean()

def ordinal_normalization(X):
    return (X.rank() - 1) / (X.max() - 1)

def frequency_normalization(X):
    return X.groupby(X).transform('count') / X.count()

def pareto_scaling_normalization(X):
    return (X - X.mean()) / np.sqrt(X.std())

def variable_stability_scaling_normalization(X):
    return (X - X.mean()) / X.std() * (X.mean() / X.std())

def power_transformation_normalization(X):
    return np.sqrt(X - X.min()) - np.sqrt(X.mean())

def maximum_normalization(X):
    return X / np.max(np.abs(X))

def logistic_sigmoid_normalization(X, eta=1):
    z = (X - X.mean()) / (eta * X.std())
    return 1 / (1 + np.exp(-z))

def hyperbolic_tangent_normalization(X):
    z = (X - X.mean()) / X.std()
    return (1 - np.exp(-z)) / (1 + np.exp(-z))

```

可视化

```python
def visualize_normalization(original_data, normalized_data, method_name, save_path=None):
    '''
    可视化原始数据和归一化数据
    :param original_data: DataFrame, 原始数据.
    :param normalized_data:  归一化数据.
    :param method_name: 方法名.
    :param save_path: 路径
    :return:
    '''

    columns_to_normalize = original_data.columns

    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), dpi=300)

    # Original data
    for i, col in enumerate(columns_to_normalize):
        axes[0].scatter(original_data.index, original_data[col], label=col)
    axes[0].set_title('data')
    axes[0].legend()

    # Normalized data
    for i, col in enumerate(columns_to_normalize):
        axes[1].scatter(original_data.index, normalized_data[col], label=col)
    axes[1].set_title(f'{method_name} normalization')
    axes[1].legend()

    plt.tight_layout()
    # 保存图表（如果指定了保存路径）
    if save_path:
        plt.savefig(save_path, dpi=300)  # 设置dpi参数以调整分辨率
    else:
        plt.show()

data = pd.DataFrame({
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [10, 20, 30, 40, 50],
    'feature3': [100, 200, 300, 400, 500]
})

# Min-Max标准化
normalized_data_min_max = data.apply(min_max_normalization)
# Mean-Std标准化
normalized_data_mean_std = data.apply(mean_std_normalization)
# Mean-mean_iqr标准化
normalized_data_mean_iqr = data.apply(mean_iqr_normalization)
# Mean-mad标准化
normalized_data_mean_mad = data.apply(median_mad_normalization)
# Mean标准化
normalized_data_Mean = data.apply(mean_normalization)
# ordinal标准化
normalized_data_ordinal = data.apply(ordinal_normalization)
# frequency标准化
normalized_data_frequency = data.apply(frequency_normalization)
# pareto_scaling标准化
normalized_data_pareto_scaling = data.apply(pareto_scaling_normalization)
# variable_stability_scaling标准化
normalized_data_variable_stability_scaling = data.apply(variable_stability_scaling_normalization)
# power_transformation标准化
normalized_data_power = data.apply(power_transformation_normalization)
# maximum_transformation标准化
normalized_data_maximum = data.apply(maximum_normalization)
# logistic_sigmoid_transformation标准化
normalized_data_logistic_sigmoid = data.apply(logistic_sigmoid_normalization)
# hyperbolic_tangent_transformation标准化
normalized_data_hyperbolic_tangent = data.apply(hyperbolic_tangent_normalization)

path = 'D:\\code\\blog\\assets\\blog_res\\2024-1-10-数据标准化\\'
# 可视化Min-Max标准化
visualize_normalization(data, normalized_data_min_max, 'Min-Max',path)
# 可视化Mean-Std标准化
visualize_normalization(data, normalized_data_mean_std, 'Mean-Std',path)
# 可视化Mean-mean_iqr标准化
visualize_normalization(data, normalized_data_mean_iqr, 'Mean-mean_iqr',path)
# 可视化Mean-mad标准化
visualize_normalization(data, normalized_data_mean_mad, 'Mean-mad',path)
# 可视化Mean标准化
visualize_normalization(data, normalized_data_Mean, 'Mean',path)
# 可视化ordinal标准化
visualize_normalization(data, normalized_data_ordinal, 'ordinal',path)
# 可视化frequency标准化
visualize_normalization(data, normalized_data_frequency, 'frequency',path)
# 可视化pareto_scaling标准化
visualize_normalization(data, normalized_data_pareto_scaling, 'pareto_scaling',path)
# 可视化variable_stability_scaling标准化
visualize_normalization(data, normalized_data_variable_stability_scaling, 'variable_stability_scaling',path)
# 可视化power_transformation标准化
visualize_normalization(data, normalized_data_power, 'power_transformation',path)
# 可视化maximum_transformation标准化
visualize_normalization(data, normalized_data_maximum, 'maximum_transformation',path)
# 可视化logistic_sigmoid_transformation标准化
visualize_normalization(data, normalized_data_logistic_sigmoid, 'logistic_sigmoid_transformation',path)
# 可视化hyperbolic_tangent_transformation标准化
visualize_normalization(data, normalized_data_hyperbolic_tangent, 'hyperbolic_tangent_transformation',path)
```



![Mean-Std](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean-Std.png)![Mean-mad](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean-mad.png)![Mean](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\Mean.png)![ordinal](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\ordinal.png)![frequency](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\frequency.png)![pareto_scaling](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\pareto_scaling.png)![variable_stability_scaling](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\variable_stability_scaling.png)![variable_stability_scaling](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\variable_stability_scaling.png)![maximum_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\maximum_transformation.png)![logistic_sigmoid_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\logistic_sigmoid_transformation.png)![hyperbolic_tangent_transformation](D:\code\blog\assets\blog_res\2024-1-10-数据标准化\hyperbolic_tangent_transformation.png)